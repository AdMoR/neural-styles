{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b76e8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: lightning in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (1.8.4.post0)\n",
      "Requirement already satisfied, skipping upgrade: deepdiff>=5.7.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (6.2.1)\n",
      "Requirement already satisfied, skipping upgrade: arrow>=1.2.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (1.2.3)\n",
      "Requirement already satisfied, skipping upgrade: tensorboardX>=2.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (2.5.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.9.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (1.23.5)\n",
      "Requirement already satisfied, skipping upgrade: lightning-utilities!=0.4.0,>=0.3.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (0.4.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=17.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (22.0)\n",
      "Requirement already satisfied, skipping upgrade: fsspec>=2022.5.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (2022.11.0)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=5.4 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (6.0)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp>=3.8.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: torchmetrics>=0.7.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: inquirer>=2.10.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.57.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (4.64.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=4.0.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=5.3.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (5.7.0)\n",
      "Requirement already satisfied, skipping upgrade: starsessions<2.0,>=1.2.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (8.1.3)\n",
      "Requirement already satisfied, skipping upgrade: lightning-cloud>=0.5.12 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (0.5.12)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4>=4.8.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (4.11.1)\n",
      "Requirement already satisfied, skipping upgrade: psutil in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (5.9.4)\n",
      "Requirement already satisfied, skipping upgrade: croniter<1.4.0,>=1.3.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning) (1.3.8)\n",
      "Requirement already satisfied, skipping upgrade: ordered-set<4.2.0,>=4.0.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from deepdiff>=5.7.0->lightning) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from arrow>=1.2.0->lightning) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf<=3.20.1,>=3.8.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from tensorboardX>=2.2->lightning) (3.20.1)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-runtime-cu11==11.7.99 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch>=1.9.0->lightning) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch>=1.9.0->lightning) (11.7.99)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cudnn-cu11==8.5.0.96 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch>=1.9.0->lightning) (8.5.0.96)\n",
      "Requirement already satisfied, skipping upgrade: nvidia-cublas-cu11==11.10.3.66 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch>=1.9.0->lightning) (11.10.3.66)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<5.0,>=4.0.0a3 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (4.0.2)\n",
      "Requirement already satisfied, skipping upgrade: charset-normalizer<3.0,>=2.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (6.0.3)\n",
      "Requirement already satisfied, skipping upgrade: aiosignal>=1.1.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (1.8.2)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (22.1.0)\n",
      "Requirement already satisfied, skipping upgrade: frozenlist>=1.1.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from aiohttp>=3.8.0->lightning) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: readchar>=3.0.6 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from inquirer>=2.10.0->lightning) (4.0.3)\n",
      "Requirement already satisfied, skipping upgrade: python-editor>=1.0.4 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from inquirer>=2.10.0->lightning) (1.0.4)\n",
      "Requirement already satisfied, skipping upgrade: blessed>=1.19.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from inquirer>=2.10.0->lightning) (1.19.1)\n",
      "Requirement already satisfied, skipping upgrade: starlette<1,>=0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from starsessions<2.0,>=1.2.1->lightning) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous<3.0.0,>=2.0.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (1.26.13)\n",
      "Requirement already satisfied, skipping upgrade: rich in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (12.6.0)\n",
      "Requirement already satisfied, skipping upgrade: fastapi[all] in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (0.88.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: pyjwt in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from lightning-cloud>=0.5.12->lightning) (2.28.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>1.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from beautifulsoup4>=4.8.0->lightning) (2.3.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: wheel in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from nvidia-cuda-runtime-cu11==11.7.99->torch>=1.9.0->lightning) (0.38.4)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from nvidia-cuda-runtime-cu11==11.7.99->torch>=1.9.0->lightning) (44.0.0)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.8.0->lightning) (3.4)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth>=0.1.4 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from blessed>=1.19.0->inquirer>=2.10.0->lightning) (0.2.5)\n",
      "Requirement already satisfied, skipping upgrade: anyio<5,>=3.4.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from starlette<1,>=0->starsessions<2.0,>=1.2.1->lightning) (3.6.2)\n",
      "Requirement already satisfied, skipping upgrade: pygments<3.0.0,>=2.6.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from rich->lightning-cloud>=0.5.12->lightning) (2.13.0)\n",
      "Requirement already satisfied, skipping upgrade: commonmark<0.10.0,>=0.9.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from rich->lightning-cloud>=0.5.12->lightning) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (1.10.2)\n",
      "Requirement already satisfied, skipping upgrade: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (5.6.0)\n",
      "Requirement already satisfied, skipping upgrade: httpx>=0.23.0; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: email-validator>=1.1.1; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: python-multipart>=0.0.5; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.0.5)\n",
      "Requirement already satisfied, skipping upgrade: uvicorn[standard]>=0.12.0; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.20.0)\n",
      "Requirement already satisfied, skipping upgrade: jinja2>=2.11.2; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (3.1.2)\n",
      "Requirement already satisfied, skipping upgrade: orjson>=3.2.1; extra == \"all\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from fastapi[all]->lightning-cloud>=0.5.12->lightning) (3.8.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from requests->lightning-cloud>=0.5.12->lightning) (2022.12.7)\n",
      "Requirement already satisfied, skipping upgrade: sniffio>=1.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<1,>=0->starsessions<2.0,>=1.2.1->lightning) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: rfc3986[idna2008]<2,>=1.3 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from httpx>=0.23.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: httpcore<0.17.0,>=0.15.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from httpx>=0.23.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: dnspython>=1.15.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from email-validator>=1.1.1; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (2.2.1)\n",
      "Requirement already satisfied, skipping upgrade: h11>=0.8 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.14.0)\n",
      "Requirement already satisfied, skipping upgrade: watchfiles>=0.13; extra == \"standard\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.18.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dotenv>=0.13; extra == \"standard\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.21.0)\n",
      "Requirement already satisfied, skipping upgrade: uvloop!=0.15.0,!=0.15.1,>=0.14.0; sys_platform != \"win32\" and (sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\") and extra == \"standard\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: websockets>=10.4; extra == \"standard\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (10.4)\n",
      "Requirement already satisfied, skipping upgrade: httptools>=0.5.0; extra == \"standard\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=2.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from jinja2>=2.11.2; extra == \"all\"->fastapi[all]->lightning-cloud>=0.5.12->lightning) (2.1.1)\n",
      "Requirement already satisfied: torchvision in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (0.14.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: requests in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==1.13.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torchvision) (1.13.0)\n",
      "Requirement already satisfied: numpy in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torchvision) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from requests->torchvision) (1.26.13)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from torch==1.13.0->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from nvidia-cuda-runtime-cu11==11.7.99->torch==1.13.0->torchvision) (44.0.0)\n",
      "Requirement already satisfied: wheel in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from nvidia-cuda-runtime-cu11==11.7.99->torch==1.13.0->torchvision) (0.38.4)\n",
      "Requirement already satisfied: pandas in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages (from scikit-learn) (1.9.3)\n",
      "Collecting filetype\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Installing collected packages: filetype\n",
      "Successfully installed filetype-1.2.0\n"
     ]
    }
   ],
   "source": [
    "#!pip uninstall pytorch_lightning -y\n",
    "!pip install lightning --upgrade\n",
    "!pip install torchvision\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd063883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import lightning as pl\n",
    "\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03cf3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_dir = os.listdir(\"/media/amor/b30209c0-758e-49ec-ae71-e14cf76352f2/home/amor/db/\")\n",
    "\n",
    "#classes_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7884f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import filetype\n",
    "\n",
    "\n",
    "def filter_non_images(image_list):\n",
    "    return list(filter(filetype.is_image, image_list))\n",
    "\n",
    "\n",
    "class NSFWDataset(Dataset):\n",
    "    \"\"\" Cassava Dataset \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir=\"/media/amor/b30209c0-758e-49ec-ae71-e14cf76352f2/home/amor/db/\", \n",
    "                 transform=transforms.Compose([\n",
    "                    transforms.RandomPerspective(fill=0, p=1, distortion_scale=0.3),\n",
    "                    transforms.RandomResizedCrop(224, ratio=(0.9, 1.1)),\n",
    "                    ToTensor()]), \n",
    "                 stage=None):\n",
    "        classes_dir = os.listdir(root_dir)\n",
    "        self.class_to_index = dict()\n",
    "        self.image_locations = list()\n",
    "        self.labels = list()\n",
    "        for i, class_loc in enumerate(classes_dir):\n",
    "            class_path = os.path.join(root_dir, class_loc)\n",
    "            image_paths = filter_non_images(\n",
    "                [os.path.join(class_path, x) \n",
    "                 for x in os.listdir(class_path)]\n",
    "            )\n",
    "            self.image_locations.extend(image_paths)\n",
    "            self.labels.extend([i for _ in range(len(image_paths))])\n",
    "            self.class_to_index[class_loc] = i\n",
    "            \n",
    "        train_images, test_images, train_labels, test_labels = \\\n",
    "            model_selection.train_test_split(self.image_locations, self.labels,\n",
    "                                             shuffle=True, random_state=4)\n",
    "            \n",
    "        if stage is not None and stage.lower() == \"train\":\n",
    "            self.image_locations = train_images\n",
    "            self.labels = train_labels\n",
    "        elif stage is not None and stage.lower() == \"test\":\n",
    "            self.image_locations = test_images\n",
    "            self.labels = test_labels\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_locations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get and load image\n",
    "        image_path = self.image_locations[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        # Perform transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Get label\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73177381",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dts \u001b[38;5;241m=\u001b[39m \u001b[43mNSFWDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dts\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m100000\u001b[39m), torch\u001b[38;5;241m.\u001b[39mmean(dts\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m100000\u001b[39m)[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(dts\u001b[38;5;241m.\u001b[39mclass_to_index)\n",
      "Cell \u001b[0;32mIn[68], line 27\u001b[0m, in \u001b[0;36mNSFWDataset.__init__\u001b[0;34m(self, root_dir, transform, stage)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, class_loc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes_dir):\n\u001b[1;32m     26\u001b[0m     class_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, class_loc)\n\u001b[0;32m---> 27\u001b[0m     image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_non_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_locations\u001b[38;5;241m.\u001b[39mextend(image_paths)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mextend([i \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_paths))])\n",
      "Cell \u001b[0;32mIn[68], line 9\u001b[0m, in \u001b[0;36mfilter_non_images\u001b[0;34m(image_list)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_non_images\u001b[39m(image_list):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiletype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/filetype/helpers.py:60\u001b[0m, in \u001b[0;36mis_image\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_image\u001b[39m(obj):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Checks if a given input is a supported type image.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m        TypeError: if obj is not a supported type.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimage_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/filetype/match.py:53\u001b[0m, in \u001b[0;36mimage_match\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimage_match\u001b[39m(obj):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Matches the given input against the available\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    image type matchers.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m        TypeError: if obj is not a supported type.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_matchers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/filetype/match.py:30\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(obj, matchers)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(obj, matchers\u001b[38;5;241m=\u001b[39mTYPES):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    Matches the given input against the available\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    file type matchers.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        TypeError: if obj is not a supported type.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[43mget_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m matcher \u001b[38;5;129;01min\u001b[39;00m matchers:\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m matcher\u001b[38;5;241m.\u001b[39mmatch(buf):\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/filetype/utils.py:64\u001b[0m, in \u001b[0;36mget_bytes\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signature(obj)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_signature_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signature(obj)\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/filetype/utils.py:25\u001b[0m, in \u001b[0;36mget_signature_bytes\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mReads file from disk and returns the first 8192 bytes\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mof data representing the magic number header signature.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    First 8192 bytes of the file content as bytearray type.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytearray\u001b[39m(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_NUM_SIGNATURE_BYTES\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dts = NSFWDataset()\n",
    "\n",
    "dts.__getitem__(100000), torch.mean(dts.__getitem__(100000)[0]), len(dts.class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5eab92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "d = datasets.ImageFolder(root=\"/media/amor/b30209c0-758e-49ec-ae71-e14cf76352f2/home/amor/db/\", \n",
    "                     transform=transforms.Compose([\n",
    "                        transforms.RandomPerspective(fill=0, p=1, distortion_scale=0.3),\n",
    "                        transforms.RandomResizedCrop(224, ratio=(0.9, 1.1)),\n",
    "                        ToTensor()]), \n",
    "                     is_valid_file=filetype.is_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "05c77f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(d.samples, open(\"valid_img_samples.json\", \"w\"))\n",
    "json.dump(d.class_to_idx, open(\"class_to_idx.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "117d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class ModelTrainer(pl.LightningModule):\n",
    "    def __init__(self, n_classes=67):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "        self.model.fc = torch.nn.Linear(512, n_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "82ad1766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_format_transform_repr',\n",
       " '_is_protocol',\n",
       " '_repr_indent',\n",
       " 'class_to_idx',\n",
       " 'classes',\n",
       " 'extensions',\n",
       " 'extra_repr',\n",
       " 'find_classes',\n",
       " 'imgs',\n",
       " 'loader',\n",
       " 'make_dataset',\n",
       " 'root',\n",
       " 'samples',\n",
       " 'target_transform',\n",
       " 'targets',\n",
       " 'transform',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd010d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Missing logger folder: /home/amor/Documents/code_dw/neural-styles/notebooks/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | model | ResNet | 11.2 M\n",
      "---------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.844    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9843f94c205b4b70998b9106b87d65c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 5.\nOriginal Traceback (most recent call last):\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torchvision/datasets/folder.py\", line 248, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/PIL/Image.py\", line 921, in convert\n    self.load()\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/PIL/ImageFile.py\", line 254, in load\n    raise OSError(\nOSError: image file is truncated (1 bytes not processed)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m nsfw_model \u001b[38;5;241m=\u001b[39m ModelTrainer(\u001b[38;5;28mlen\u001b[39m(dts\u001b[38;5;241m.\u001b[39mclass_to_index))\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnsfw_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m           \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m           \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:603\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 603\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:645\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    638\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    641\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    643\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    644\u001b[0m )\n\u001b[0;32m--> 645\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1098\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1098\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1177\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1177\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/trainer.py:1200\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/loops/epoch/training_epoch_loop.py:188\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    187\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 188\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     batch_idx, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/supporters.py:568\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning/pytorch/trainer/supporters.py:580\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/lightning_utilities/core/apply_func.py:47\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Breaking condition\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype) \u001b[38;5;129;01mand\u001b[39;00m (wrong_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m elem_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(data)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1313\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1312\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 5.\nOriginal Traceback (most recent call last):\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torchvision/datasets/folder.py\", line 229, in __getitem__\n    sample = self.loader(path)\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torchvision/datasets/folder.py\", line 268, in default_loader\n    return pil_loader(path)\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/torchvision/datasets/folder.py\", line 248, in pil_loader\n    return img.convert(\"RGB\")\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/PIL/Image.py\", line 921, in convert\n    self.load()\n  File \"/home/amor/Documents/code_dw/neural-styles/venv39/lib/python3.9/site-packages/PIL/ImageFile.py\", line 254, in load\n    raise OSError(\nOSError: image file is truncated (1 bytes not processed)\n"
     ]
    }
   ],
   "source": [
    "nsfw_model = ModelTrainer(len(dts.class_to_index))\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\")\n",
    "trainer.fit(model=nsfw_model, \n",
    "            train_dataloaders=DataLoader(d, batch_size=64, shuffle=True, \n",
    "                                         num_workers=8, pin_memory=True ),\n",
    "           \n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e37b09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53b2fc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df8ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
